{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7857df2a-31fa-494f-b37e-86c1db94a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "\n",
    "import glob\n",
    "import time\n",
    "import re\n",
    "import regionmask\n",
    "import datetime\n",
    "# to fix MJD\n",
    "import astropy\n",
    "from astropy.time import Time\n",
    "\n",
    "# data management\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.plot import show as rioshow\n",
    "from shapely.geometry import Polygon\n",
    "from geospatial_functions import get_background_map\n",
    "\n",
    "# imported from provided py code: long functions\n",
    "from calc_geoid_change_alt import readstrokescoefficients, plm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3381d043-3d00-47e4-9702-3a70387e159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading other layers\n",
    "outline           = gpd.read_file(f\"Data\\\\lena_basin_outline_polygon.gpkg\",driver=\"GPKG\")\n",
    "main_rivers       = gpd.read_file(f\"Data\\\\lena_main_river.gpkg\",driver=\"GPKG\")   \n",
    "coast             = gpd.read_file(f\"Data\\\\north_east_russian_coastlines.gpkg\",driver=\"GPKG\")   \n",
    "# fixing crs\n",
    "for layer in [outline,main_rivers,coast]:\n",
    "    layer.geometry = layer.geometry.to_crs(\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb62148-5288-4f3f-917f-0b90e41eb536",
   "metadata": {},
   "source": [
    "background:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf38b6d-4bc0-46c1-8dac-46e696bfee97",
   "metadata": {},
   "source": [
    "Grace can measure $\\frac{\\Delta S}{\\Delta t}$, when looking at the water balance: $\\frac{\\Delta S}{\\Delta t} = P - L$ where $L$ are the losses.\n",
    "\n",
    "Losses are due to Evapotranspiration ($ET$), Discharge ($Q$) and Underground flow (ground water - $G$). \n",
    "\n",
    "Evaporation can be esimated but is difficult, discharge can be considered known: \n",
    "\n",
    "$\\frac{\\Delta S}{\\Delta t} = P - Q - ET - GW$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cc6abb-15cb-4123-a954-10425a0e658f",
   "metadata": {},
   "source": [
    "# focus on grace data\n",
    "Downloaded data can be loaded in, note using stokes coefficients till 60th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "524c4e05-4769-4d23-959e-70b54bc8b0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "grace_files = glob.glob('Data\\\\Grace_filtered\\*.gfc')\n",
    "love_numbers_kl = np.loadtxt('Data\\\\loadLoveNumbers_60.txt')[:,1]\n",
    "l = 60\n",
    "m = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cb9398-3049-40d4-9e4a-59bedd4ec7f5",
   "metadata": {},
   "source": [
    "Stokes coeffcients order 1& 2 need to be handeled differently\n",
    "\n",
    "order 1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81738dc2-fe97-4479-b75b-d186634a7015",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_stoke_coeff1 = f'Data\\\\Grace\\\\degree1_stokes_coeff.txt'\n",
    "df_stokes_1 = pd.read_csv(fname_stoke_coeff1,skiprows=116,delimiter=\"\\s+\", \n",
    "        names=['GRCOF2',\"l\",\"m\", \"Clm\",\"Slm\",\"sd_Clm\",\"sd_Slm\",\"begin_date\",\"end_date\"])\n",
    "### drop unwanted\n",
    "df_stokes_1.drop(columns=[\"GRCOF2\",\"sd_Clm\",\"sd_Slm\"],inplace=True)\n",
    "### Reformat dates\n",
    "df_stokes_1[\"end_date\"] = df_stokes_1.apply(lambda x: pd.Timestamp(f'{str(x.end_date)[0:4]}-{str(x.end_date)[4:6]}-{str(x.end_date)[6:8]}'),axis=1)\n",
    "### dirty fix to make more fit\n",
    "df_stokes_1[\"begin_date\"] = df_stokes_1.apply(lambda x: pd.Timestamp(f'{str(x.begin_date)[0:4]}-{str(x.begin_date)[4:6]}-01'),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "536518bb-2a6f-48e8-9c5c-31cae3d9bf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_0 = df_stokes_1[df_stokes_1['m'] == 0].set_index('begin_date')\n",
    "df_1_1 = df_stokes_1[df_stokes_1['m'] == 1].set_index('begin_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5621d2d9-215d-414e-9d4f-8b5468655edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# easiest way to fix the indexing in matchting the correct indexes\n",
    "df_index_replace = df_1_1.index.to_numpy().copy()\n",
    "for i, index in enumerate(df_1_1.index):\n",
    "    if i == 0:\n",
    "        pass\n",
    "    if (df_1_1.index[i-1] - index) == pd.Timedelta(0):\n",
    "        replace = pd.Timestamp(f'{index.year}-{index.month+1}-{index.day}')\n",
    "        df_index_replace[i] = replace\n",
    "df_1_1.index = df_index_replace\n",
    "df_1_0.index = df_index_replace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e8c994-e032-4cd4-a919-7d25e4e577df",
   "metadata": {},
   "source": [
    "order 2,0 & 3,0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c240b506-2395-42e7-8614-0b00667c96da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MJD_to_ts(mjd):\n",
    "    ## thanks to https://stackoverflow.com/questions/72597699/how-do-i-convert-mjdjulian-date-to-utc-using-astropy\n",
    "    # Start with some time in modified julian date (MJD)\n",
    "    # Convert to Julian Date\n",
    "    mjd = float(mjd)\n",
    "    jd = mjd + 2400000.5\n",
    "    # Convert to astropy Time object\n",
    "    t = astropy.time.Time(jd, format='jd')\n",
    "    # Convert to datetime\n",
    "    str = t.to_datetime()\n",
    "    return str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be465350-bf18-4e21-ba84-93137812180f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_stoke_coeff2 = f'Data\\\\Grace\\\\degree2_stokes_coeff.txt'\n",
    "col_names = ['MJD begin',\"Year fraction begin\",\"C20\",\"C20 - C20_mean (1.0E-10)\",\"sig_C20 (1.0E-10)\", \n",
    "             \"C30\",\"C30 - C30_mean (1.0E-10)\",\"sig_C30 (1.0E-10)\",'MJD end',\"Year fraction end\"]\n",
    "df_stokes_2_3 = pd.read_csv(fname_stoke_coeff2,skiprows=37,delimiter=\"\\s+\",names=col_names)\n",
    "# fix date format\n",
    "df_stokes_2_3[\"begin_date\"] = df_stokes_2_3.apply(lambda x: MJD_to_ts(x['MJD begin']), axis=1)\n",
    "df_stokes_2_3[\"end_date\"] = df_stokes_2_3.apply(lambda x: MJD_to_ts(x['MJD begin']), axis=1)\n",
    "df_stokes_2_3 = df_stokes_2_3[[\"begin_date\",\"C20\",\"C30\",\"end_date\"]].set_index(\"begin_date\")\n",
    "\n",
    "# allign indexes and replace like in C_1_1..\n",
    "df_stokes_2_3 = df_stokes_2_3.iloc[:-2] # remove last two months to make same length\n",
    "df_stokes_2_3.index = df_index_replace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68bf972-b147-4577-a779-1b4335b110ed",
   "metadata": {},
   "source": [
    "Add trend due to Glacial isostatic adjustment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56242704-6c88-4c9e-81cb-28e44a23ecef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_gia = f'Data\\\\GIA\\\\GIA_stoke_coeff_trend.gz'\n",
    "df_1 = pd.read_csv(fname_gia, compression='gzip',skiprows=1,nrows=6,names=[\"l\",\"m\",\"Clm\",\"Slm\"],delimiter=\"\\s+\")\n",
    "df_2 = pd.read_csv(fname_gia, compression='gzip',skiprows=9,nrows=6,names=[\"l\",\"m\",\"Clm\",\"Slm\"],delimiter=\"\\s+\")\n",
    "df_3 = pd.read_csv(fname_gia, compression='gzip',skiprows=17,names=[\"l\",\"m\",\"Clm\",\"Slm\"],delimiter=\"\\s+\")\n",
    "\n",
    "df_combined = pd.concat([df_2, df_3])\n",
    "df_combined[\"l\"] = df_combined[\"l\"].astype(int)\n",
    "df_combined[\"m\"] = df_combined[\"m\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "306ccc58-ae15-4d12-a18f-3252e0f86734",
   "metadata": {},
   "outputs": [],
   "source": [
    "GIA_C = np.zeros((l+1, m+1))\n",
    "GIA_S = np.zeros((l+1, m+1))\n",
    "for index, row in df_combined.iterrows():\n",
    "    if row.m <= m and row.l <= l:\n",
    "        GIA_C[int(row.m),int(row.l)] = row[\"Clm\"]\n",
    "        GIA_S[int(row.m),int(row.l)] = row[\"Slm\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044cceb1-8b71-4669-9f05-50dbc112bda3",
   "metadata": {},
   "source": [
    "Adjust readstokescoefficients to also obtain `time_period_of_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ed577d2-7e25-42e6-b1de-0309d53dd106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readstrokescoefficients_filtered(filename):\n",
    "    with open(filename) as f:\n",
    "        reach_end_of_head = 0\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if reach_end_of_head == 0:\n",
    "                if line.startswith(\"earth_gravity_constant\"):\n",
    "                    GM = float(line[line.index(\" \") + 1:])\n",
    "                elif \"radius\" in line:\n",
    "                    line = re.sub(' +', ' ', line)\n",
    "                    R = float(line.split(\" \")[1])\n",
    "                elif line.startswith(\"max_degree\"):\n",
    "                    line = re.sub(' +', ' ', line)\n",
    "                    max_degree = int(line.split(\" \")[1])\n",
    "                    C = np.zeros((max_degree + 1, max_degree + 1))\n",
    "                    S = np.zeros((max_degree + 1, max_degree + 1))\n",
    "                elif line.startswith('time_period_of_data'):\n",
    "                    line = re.sub(' +', ' ', line)\n",
    "                    date_line = line.split(\" \")\n",
    "                    begin_date = str(line.split(\" \")[1])\n",
    "                    begin_date_ts = pd.Timestamp(f'{begin_date[0:4]}-{begin_date[4:6]}-01')\n",
    "                    mid_date = end_date = str(line.split(\" \")[-1])\n",
    "                    mid_date_ts = pd.Timestamp(f'{mid_date[0:4]}-{mid_date[4:6]}-{mid_date[6:8]}')\n",
    "                    end_date = str(line.split(\" \")[3])\n",
    "                    end_date_ts = pd.Timestamp(f'{end_date[0:4]}-{end_date[4:6]}-{end_date[6:8]}')\n",
    "                    date_lst = [begin_date_ts,mid_date_ts,end_date_ts]\n",
    "                    \n",
    "                else:\n",
    "                    if line.startswith(\"end_of_head\"):\n",
    "                        reach_end_of_head = 1\n",
    "            else:\n",
    "                line = re.sub(' +', ' ', line)\n",
    "                line = line.split()\n",
    "                L = int(line[1])\n",
    "                M = int(line[2])\n",
    "                C[L, M] = float(line[3])\n",
    "                S[L, M] = float(line[4])\n",
    "    return C, S, R, GM, date_lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "857d684a-a022-4bb3-84b3-feb948c75133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in all coefficient\n",
    "C, S, times, grace_names = [], [], [], []\n",
    "plot_times = []\n",
    "for i, file in enumerate(grace_files):\n",
    "    C_i, S_i, R, GM, [begin_date_ts, mid_date_ts, end_date_ts] = readstrokescoefficients_filtered(file)\n",
    "    time_i = begin_date_ts\n",
    "    # replace C_1_0,C_1_1, S_1_1, C_2_0, C_3_0\n",
    "    try:\n",
    "        test  = df_1_0.loc[time_i,\"Clm\"]\n",
    "        new_time = time_i\n",
    "    except KeyError: # issue with finding correct value, this is easiest fix\n",
    "        new_time = pd.Timestamp(f'{time_i.year}-{time_i.month-1}-{time_i.day}')\n",
    "        print(new_time)\n",
    "    \n",
    "    C_i[1,0]  = df_1_0.loc[new_time,\"Clm\"]\n",
    "    C_i[1,1]  = df_1_1.loc[new_time,\"Clm\"]\n",
    "    S_i[1,1]  = df_1_1.loc[new_time,\"Slm\"]\n",
    "    C_i[2,0]  = df_stokes_2_3.loc[new_time,\"C20\"]\n",
    "    C_30  = df_stokes_2_3.loc[new_time,\"C30\"]\n",
    "    # print(C_30)\n",
    "    if np.isnan(C_30): \n",
    "        pass\n",
    "    else: \n",
    "        C_i[3,0] = C_30\n",
    "\n",
    "    # add glacia isostatic adjustment\n",
    "    C_i += GIA_C\n",
    "    S_i += GIA_S\n",
    "    \n",
    "    C.append(C_i), S.append(S_i), times.append(time_i), grace_names.append(f'{time_i.year}-{time_i.month}-{time_i.day}'), plot_times.append(mid_date_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6296842e-5062-4e89-9499-9e62620d7923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.00048\n"
     ]
    }
   ],
   "source": [
    "print(f'{C[0][2,0]:.2g}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "135ee379-6137-4b6f-b3c4-529281e7a7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Data\\\\grace_names_friendly.txt\",np.array(grace_names),delimiter=\" \", fmt=\"%s\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20d4e510-042a-44f1-83d2-4e34f2d7ea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate means of coefficients\n",
    "C = np.array(C)\n",
    "S = np.array(S)\n",
    "C_mean = C.sum(axis=0)/len(C)\n",
    "S_mean = S.sum(axis=0)/len(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ae106bc-dc6d-4d3d-902d-df818307ef2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove mean coefficients\n",
    "dc1_store = []\n",
    "ds1_store = []\n",
    "for i, c in enumerate(C):\n",
    "    dc1 = C[i] - C_mean\n",
    "    ds1 = S[i] - S_mean\n",
    "    dc1_store.append(dc1)\n",
    "    ds1_store.append(ds1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6772a4f4-eb40-442e-accd-f0dd1bc91b06",
   "metadata": {},
   "source": [
    "using \n",
    "\\begin{equation}\n",
    "\\delta h_w \\left(\\theta,\\lambda\\right) = \\sum_{l,m=0}^{\\infty}=\\bar{C_{lm}^{\\delta h_w}} \\bar{Y}_{lm}\\left(\\theta,\\lambda\\right)\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60b8f7cf-74ae-46bb-93b8-4b8cad9a0ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rho_av__rho_w = 5.5 # aprox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a594984b-e0b7-4088-9bda-50da04317cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dh_c1_store = []\n",
    "dh_s1_store = []\n",
    "\n",
    "for k, dc1 in enumerate(dc1_store):\n",
    "    C_dhw_i_1 = np.zeros((l+1, l+1))\n",
    "    S_dhw_i_1 = np.zeros((l+1, l+1))\n",
    "    for i in range(l+1):\n",
    "        for j in range(i+1):\n",
    "            multiplication_factor = (R  * (2 * i + 1) * rho_av__rho_w) / ( 3 * (1 + love_numbers_kl[i]))\n",
    "            C_dhw_i_1[i, j] = (dc1_store[k][i, j] * multiplication_factor)\n",
    "            S_dhw_i_1[i, j] = (ds1_store[k][i, j] * multiplication_factor)\n",
    "    dh_c1_store.append(C_dhw_i_1)\n",
    "    dh_s1_store.append(S_dhw_i_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312fc3c3-0e6c-41df-aa05-aab739d6a590",
   "metadata": {},
   "source": [
    "Create array of lat lon for the area: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c418938-806b-4219-adbe-b16c2e7ec9ec",
   "metadata": {},
   "source": [
    "![Figures\\Lena_Basin_map.png](Figures\\Lena_Basin_map.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b938f906-5f05-43d5-a8f3-430e35cbc6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_lambda= [ 90.  91.  92.  93.  94.  95.  96.  97.  98.  99. 100. 101. 102. 103.\n",
      " 104. 105. 106. 107. 108. 109. 110. 111. 112. 113. 114. 115. 116. 117.\n",
      " 118. 119. 120. 121. 122. 123. 124. 125. 126. 127. 128. 129. 130. 131.\n",
      " 132. 133. 134. 135. 136. 137. 138. 139. 140. 141. 142. 143. 144. 145.\n",
      " 146. 147. 148. 149.]\n",
      "\n",
      "\n",
      "theta= [40. 39. 38. 37. 36. 35. 34. 33. 32. 31. 30. 29. 28. 27. 26. 25. 24. 23.\n",
      " 22. 21. 20. 19. 18. 17. 16. 15. 14. 13. 12. 11.]\n"
     ]
    }
   ],
   "source": [
    "_lambda = np.pi/180 * np.arange(270,330,1) - np.pi # 90 - 150 # deg lon\n",
    "theta = np.pi - np.pi/180 * np.arange(180-40, 180-10,1) # 80 - 50 deg lat\n",
    "\n",
    "print('_lambda=',_lambda/np.pi*180)\n",
    "print('\\n')\n",
    "print('theta=',theta/np.pi*180)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632a3f92-33ef-41a9-9b1f-221244ffbf97",
   "metadata": {},
   "source": [
    "This is how the loop looks, to speed it up it is run in parrallel in `multicore_raw.py`, thus here we only actually load in the netCDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e007eb6-141a-45fb-bd93-2ca5fa36a89b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "did not find a match in any of xarray's currently installed IO backends ['netcdf4', 'h5netcdf', 'scipy', 'cfgrib', 'pydap', 'rasterio', 'zarr']. Consider explicitly selecting one of the installed engines via the ``engine`` parameter, or installing additional IO dependencies, see:\nhttps://docs.xarray.dev/en/stable/getting-started-guide/installing.html\nhttps://docs.xarray.dev/en/stable/user-guide/io.html",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28mprint\u001b[39m(datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow())\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 50\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ship\\Lib\\site-packages\\xarray\\backends\\api.py:510\u001b[0m, in \u001b[0;36mopen_dataset\u001b[1;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, backend_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m    507\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate(backend_kwargs)\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 510\u001b[0m     engine \u001b[38;5;241m=\u001b[39m \u001b[43mplugins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mguess_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    512\u001b[0m backend \u001b[38;5;241m=\u001b[39m plugins\u001b[38;5;241m.\u001b[39mget_backend(engine)\n\u001b[0;32m    514\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[0;32m    515\u001b[0m     decode_cf,\n\u001b[0;32m    516\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    522\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[0;32m    523\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ship\\Lib\\site-packages\\xarray\\backends\\plugins.py:177\u001b[0m, in \u001b[0;36mguess_engine\u001b[1;34m(store_spec)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound the following matches with the input file in xarray\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms IO \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    172\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackends: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompatible_engines\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. But their dependencies may not be installed, see:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.xarray.dev/en/stable/user-guide/io.html \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    174\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.xarray.dev/en/stable/getting-started-guide/installing.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    175\u001b[0m     )\n\u001b[1;32m--> 177\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n",
      "\u001b[1;31mValueError\u001b[0m: did not find a match in any of xarray's currently installed IO backends ['netcdf4', 'h5netcdf', 'scipy', 'cfgrib', 'pydap', 'rasterio', 'zarr']. Consider explicitly selecting one of the installed engines via the ``engine`` parameter, or installing additional IO dependencies, see:\nhttps://docs.xarray.dev/en/stable/getting-started-guide/installing.html\nhttps://docs.xarray.dev/en/stable/user-guide/io.html"
     ]
    }
   ],
   "source": [
    "# last run: 2023-10-26 09:42:55.218272 - 2023-10-26 10:39:23.874429\n",
    "# fname = \"Data\\\\anomally_waterhead_full.nc\"\n",
    "# fname = \"Data\\\\anomally_waterhead_filtered_multicore.nc\"\n",
    "# fname = \"Data\\\\anomally_waterhead_filtered_test2.nc\" # debug\n",
    "run = False\n",
    "debug = False\n",
    "\n",
    "if debug:\n",
    "    n=5                \n",
    "    loop = dh_c1_store[:n]\n",
    "    times = times[:n]      # debugging\n",
    "    # fname = \"Data\\\\anomally_waterhead_filtered_test.nc\"\n",
    "    if run: fname = \"Data\\\\anomally_waterhead_filtered_test.nc\"\n",
    "    else: fname = \"Data\\\\anomally_waterhead_filtered_test_multicore.nc\"\n",
    "else:\n",
    "    loop = dh_c1_store\n",
    "    if run: fname = \"Data\\\\anomally_waterhead_filtered.nc\"\n",
    "    else: fname = \"Data\\\\anomally_waterhead_filtered_multicore.nc\"\n",
    "if run:\n",
    "    # this is a much slower method, faster is the multicore.py which uses multi proecessing \n",
    "    store_ewh_1 = []\n",
    "    print(datetime.datetime.now())\n",
    "    for z, dh_c1 in enumerate(loop): # debugging\n",
    "        print(f'{z=}',end='\\n')\n",
    "        ewh_i_1 = np.zeros((len(theta), len(_lambda)))\n",
    "        ewh_i_2 = np.zeros((len(theta), len(_lambda)))\n",
    "        \n",
    "        for i in range(len(theta)):                              # loop over all thetas\n",
    "            print(f'{i=} (out of {len(theta)})',end='\\r')\n",
    "            P_lm = plm(theta[i], l)                              # all Legendre Functions for one theta\n",
    "            for j in range(len(_lambda)):                        # loop over all lambdas\n",
    "                for k in range(l+1):                             # loop over all degrees\n",
    "                    for t in range(k+1):                         # loop over negative orders\n",
    "                        sin_t_lambda = np.sin(t*_lambda[j])      # negative orders\n",
    "                        cos_t_lambda = np.cos(t*_lambda[j])      # non-negative orders\n",
    "                        # compute here equivalent water heights\n",
    "                        ewh_i_1[i, j] = ewh_i_1[i, j] + (dh_s1_store[z][k, t] * P_lm[k, t] * sin_t_lambda)\n",
    "                        ewh_i_1[i, j] = ewh_i_1[i, j] + (dh_c1_store[z][k, t] * P_lm[k, t] * cos_t_lambda)\n",
    "        print('\\r')\n",
    "        store_ewh_1.append(ewh_i_1)\n",
    "\n",
    "    # store after itteration\n",
    "    ds = xr.DataArray(store_ewh_1, dims=(\"time\",\"lat\",\"lon\"),coords={\"lon\":_lambda/np.pi*180,\n",
    "                                                                 \"lat\": 90 - theta/np.pi*180,\n",
    "                                                                 \"time\":times}, name=\"dh(m)\")\n",
    "    \n",
    "    ds.to_netcdf(fname)\n",
    "    print(datetime.datetime.now())\n",
    "else:\n",
    "    ds_in = xr.open_dataset(fname)\n",
    "    ds = ds_in.load()\n",
    "    ds_in.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa250b6d-37ef-4ca2-b01d-b3ab95d80f9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mds\u001b[49m\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ds' is not defined"
     ]
    }
   ],
   "source": [
    "ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37937255-2eb1-49c2-a53d-9d4b5a6a9101",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "i = 0\n",
    "contour_plot = ds.isel(time=i)[\"dh(m)\"].plot(ax=ax,cmap=\"viridis\")\n",
    "contour_plot.colorbar.set_label(\"Equivalent water height $\\delta h_w$ (m))\")\n",
    "\n",
    "# ds.isel(time=i).plot(ax=ax,cmap=\"viridis\")\n",
    "ax.set_title(f\"Mass anomaly {grace_names[i][:-2]}\")\n",
    "outline.plot(ax=ax, edgecolor=\"C3\", facecolor=\"None\",zorder=2)\n",
    "main_rivers.plot(ax=ax, color=\"lightskyblue\",alpha=0.6,lw=1)\n",
    "ax.set_xlabel(\"Longitude (°E)\")\n",
    "bounds = (90.0, 50.0, 150.0, 80.0)\n",
    "coast.plot(ax=ax,color='k',zorder=1)\n",
    "ax.set_xlim((bounds[0],bounds[2]))\n",
    "ax.set_ylim((bounds[1],bounds[3]));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8509b556-31b9-446f-aa99-3d571e428ff2",
   "metadata": {},
   "source": [
    "Mask the dat using [regionmask](https://regionmask.readthedocs.io/en/stable/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf2e77d-6a40-48aa-8fa5-465274f86dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = regionmask.mask_geopandas(outline, \n",
    "                                 ds.lon.to_numpy(), \n",
    "                                 ds.lat.to_numpy(),\n",
    "                                 lon_name=\"lon\",\n",
    "                                 lat_name=\"lat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3b2485-e868-4af0-902b-d76e49e503ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_lena_basin = ds.where(mask==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f07e0f-5024-4190-9e58-b3110e293234",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "# ds_lena_basin.isel(time=i).plot(ax=ax,cmap=\"viridis\")\n",
    "contour_plot = ds_lena_basin.isel(time=i)[\"dh(m)\"].plot(ax=ax,cmap=\"viridis\")\n",
    "contour_plot.colorbar.set_label(\"Equivalent water height $\\delta h_w$ (m))\")\n",
    "ax.set_title(f\"Changes between {grace_names[i]} - {grace_names[i+1]}\")\n",
    "outline.plot(ax=ax, edgecolor=\"C3\", facecolor=\"None\")\n",
    "main_rivers.plot(ax=ax, color=\"lightskyblue\",alpha=0.6,lw=1.5)\n",
    "\n",
    "bounds = (90.0, 50.0, 150.0, 80.0)\n",
    "coast.plot(ax=ax,color='k',zorder=1)\n",
    "\n",
    "ax.set_ylabel(\"Latitude (°N)\")\n",
    "ax.set_xlabel(\"Longitude (°E)\")\n",
    "## optionally add background but CRS is difficult\n",
    "\n",
    "# with rasterio.open(get_background_map(\"outline2\", bounds)) as r:\n",
    "#     rioshow(r, ax=ax,zorder=-10)\n",
    "ax.set_xlim((bounds[0],bounds[2]))\n",
    "ax.set_ylim((bounds[1],bounds[3]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7d8748-cc92-469a-b0da-f3e68bc900d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_mask = mask.values.flatten()\n",
    "masked = len(flat_mask[~np.isnan(flat_mask)])\n",
    "unmasked = len(flat_mask[np.isnan(flat_mask)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df00c932-4c78-41de-a05f-2d6a361d9c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_lambda = (_lambda[1] - _lambda[0])\n",
    "d_theta = (theta[0] - theta[1])\n",
    "print(f'spatial resolution of a gridcell is {d_lambda*180/np.pi:.2f}deg lon, {d_theta*180/np.pi:.2f}deg lat')\n",
    "mean_lambda = _lambda.mean()\n",
    "mean_theta = theta.mean()\n",
    "print(f'mean value a gridcell is {mean_lambda*180/np.pi:.2f}deg lon, {mean_theta*180/np.pi-90:.2f}deg lat')\n",
    "distance_1deg_lat = 110.574 # km\n",
    "distance_1deg_lon = 111.320 * np.cos((mean_theta*180/np.pi-90)*np.pi/180)# km\n",
    "size_1_grid_cell = distance_1deg_lon * distance_1deg_lat\n",
    "area_basin = size_1_grid_cell * masked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a651e9-610c-4621-8ec1-6c12aed2e0cf",
   "metadata": {},
   "source": [
    "Move to a timeseries by averaging over the area: i.e. mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c834dd-1518-46eb-96bb-3238866eff6c",
   "metadata": {},
   "source": [
    "In every case, grid values are weighted by cos(latitude), in order to obtain per surface unit values (cm/m²)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4a0363-b483-4e8a-b54a-5fb64d834eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "changes_total_water_mass = (ds_lena_basin.mean(dim=[\"lat\",\"lon\"])[\"dh(m)\"] * 100)\n",
    "# # fix time indexeing\n",
    "# time_shift = changes_total_water_mass.time.to_numpy()\n",
    "# changes_total_water_mass = changes_total_water_mass[2:-1]\n",
    "# changes_total_water_mass['time'] = time_shift[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a2a0a0-6ce4-423c-8d96-9dc8ea17e97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "changes_total_water_mass.plot(ax=ax,)\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Mass anomaly   $\\mu \\delta h_w$ [cm]\")\n",
    "ax.set_title(\"Mass anomalies over time in the lena basin\"); "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab27d013-515f-45dc-9651-6695495f8dd5",
   "metadata": {},
   "source": [
    "first 3 months contain a lot of error: skip these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3964ab20-a2a5-453e-9f6f-80f34989722d",
   "metadata": {},
   "outputs": [],
   "source": [
    "changes_total_water_mass = (ds_lena_basin.mean(dim=[\"lat\",\"lon\"])[\"dh(m)\"] * 100)\n",
    "changes_total_water_mass = changes_total_water_mass[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53457bbd-5d38-4a4f-9f4d-78ead43ef66b",
   "metadata": {},
   "source": [
    "Repeat but for unfiltered stoke coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8536142f-d409-454f-bf45-6db637d8b56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_unfilted = \"Data\\\\anomally_waterhead_raw_multicore.nc\"\n",
    "ds_unfilterd = xr.open_dataset(fname_unfilted)\n",
    "ds_lena_basin_unfilterd = ds_unfilterd.where(mask==0)\n",
    "changes_total_water_mass_unfilterd = (ds_lena_basin_unfilterd.mean(dim=[\"lat\",\"lon\"])[\"dh(m)\"] * 100)\n",
    "changes_total_water_mass_unfilterd = changes_total_water_mass_unfilterd[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf9d992-2ba4-4b3a-9296-b5621704512e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_finer = \"Data\\\\anomally_waterhead_filtered_multicore_finer_grid.nc\"\n",
    "ds_finer = xr.open_dataset(fname_finer)\n",
    "ds_lena_basin_finer = ds_finer.where(mask==0)\n",
    "changes_total_water_mass_finer = (ds_lena_basin_finer.mean(dim=[\"lat\",\"lon\"])[\"dh(m)\"] * 100)\n",
    "changes_total_water_mass_finer = changes_total_water_mass_finer[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec496a6-7c24-44d7-ba89-6c540cc4d925",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "# ax.plot(ds.time.to_numpy()[3:], changes_total_water_mass[2:-1],label=\"shifted\")\n",
    "changes_total_water_mass.plot(ax=ax,label=\"UTCSR - stokes coefficients - DDK4\")\n",
    "changes_total_water_mass_finer.plot(ax=ax,label=\"UTCSR - stokes coefficients - DDK4 - finer grid\",ls=\"--\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Mass anomaly   $\\mu \\delta h_w$ [cm]\")\n",
    "ax.set_title(\"Mass anomalies over time in the lena basin\"); \n",
    "ax.legend()\n",
    "fig.savefig(\"Figures\\mass anomalies over time.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf33ebc-b69f-49c5-88d4-89557ee91f64",
   "metadata": {},
   "source": [
    "reducing the grid size makes no difference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7415c2-e73e-46ba-8213-68cf0fc9b327",
   "metadata": {},
   "source": [
    "Add data from [thegraceplotter.com](thegraceplotter.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440f6466-62d1-4181-8252-08893aa788dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_to_ts(time):\n",
    "    ## thanks to https://stackoverflow.com/questions/20911015/decimal-years-to-datetime-in-python\n",
    "    year = int(time)\n",
    "    rem = time - year\n",
    "    base = datetime.datetime(year, 1, 1)\n",
    "    # result = base + datetime.timedelta(seconds=rem*365.25*24*3600) \n",
    "    result = base + datetime.timedelta(seconds=(base.replace(year=base.year + 1) - base).total_seconds() * rem)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7428376f-c6d5-4ed9-9b60-15e6a80d8866",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graceplotter = pd.read_csv(\"Data\\\\Graceplotter_data_lenabasin.txt\",delimiter=\"\\s+\",skiprows=463,names=[\"Time_S\",\"Time_D_1950\",\"Time_y\",\"EWH\",\"Lin\",\"Predic\"])\n",
    "df_graceplotter['EWH'] = df_graceplotter.apply(lambda x: float(x[\"EWH\"]) if x[\"EWH\"] != \" NaN\" else np.nan,axis=1)\n",
    "df_graceplotter = df_graceplotter.dropna()\n",
    "df_graceplotter = df_graceplotter.iloc[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de0e13b-3832-4f48-b5c3-2101354f9043",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graceplotter.index = df_graceplotter['Time_y'].apply(year_to_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a342e74-e16c-4b6a-8e0a-f2226e4fc721",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "# ax.plot(ds.time.to_numpy()[3:], changes_total_water_mass[2:-1],label=\"shifted\")\n",
    "changes_total_water_mass.plot(ax=ax,label=\"UTCSR - stokes coefficients - DDK4\")\n",
    "changes_total_water_mass_unfilterd.plot(ax=ax,label=\"TUGRAZ stokes coefficients\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Mass anomaly   $\\mu \\delta h_w$ [cm]\")\n",
    "ax.set_title(\"Mass anomalies over time in the lena basin\"); \n",
    "\n",
    "df_graceplotter[\"EWH\"].plot(ax=ax,label=\"UTCSR - The Graceplotter - DDK5\",marker=\".\",color=\"C3\")\n",
    "ax.legend()\n",
    "fig.savefig(\"Figures\\mass anomalies over time.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed05b5a3-32b9-44ab-ab85-92a72cdcb4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# changes_total_water_mass_km[:130].plot(ax=ax)\n",
    "# ax.set_xlabel(\"Date\")\n",
    "# ax.set_ylabel(\"Mass anomaly   $\\sum \\delta h_w$ [km^3]\")\n",
    "# ax.set_title(\"Mass anomalies over time in the lena basin\"); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b25821-d9e8-4785-8095-d138fdceb6d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb83dc39-7d81-4619-8b0b-3633ad6411af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cca737-b197-4145-a7ec-f2d3c54c9e83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e857ee49-08cf-47bd-8188-36cac25bb447",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
