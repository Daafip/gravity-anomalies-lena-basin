{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7857df2a-31fa-494f-b37e-86c1db94a20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\anaconda3\\envs\\ship\\Lib\\site-packages\\regionmask\\core\\regions.py:9: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas as gp\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import re\n",
    "import os\n",
    "import regionmask\n",
    "import datetime\n",
    "# to fix MJD\n",
    "import astropy\n",
    "from astropy.time import Time\n",
    "\n",
    "# data management\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "# os.environ['USE_PYGEOS'] = '0'\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.plot import show as rioshow\n",
    "from shapely.geometry import Polygon\n",
    "from geospatial_functions import get_background_map\n",
    "\n",
    "# imported from provided py code: long functions\n",
    "from calc_geoid_change_alt import readstrokescoefficients, plm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3381d043-3d00-47e4-9702-3a70387e159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading other layers\n",
    "outline           = gpd.read_file(f\"Data\\\\lena_basin_outline_polygon.gpkg\",driver=\"GPKG\")\n",
    "main_rivers       = gpd.read_file(f\"Data\\\\lena_main_river.gpkg\",driver=\"GPKG\")   \n",
    "coast             = gpd.read_file(f\"Data\\\\north_east_russian_coastlines.gpkg\",driver=\"GPKG\")   \n",
    "# fixing crs\n",
    "for layer in [outline,main_rivers,coast]:\n",
    "    layer.geometry = layer.geometry.to_crs(\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb62148-5288-4f3f-917f-0b90e41eb536",
   "metadata": {},
   "source": [
    "background:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf38b6d-4bc0-46c1-8dac-46e696bfee97",
   "metadata": {},
   "source": [
    "Grace can measure $\\frac{\\Delta S}{\\Delta t}$, when looking at the water balance: $\\frac{\\Delta S}{\\Delta t} = P - L$ where $L$ are the losses.\n",
    "\n",
    "Losses are due to Evapotranspiration ($ET$), Discharge ($Q$) and Underground flow (ground water - $G$). \n",
    "\n",
    "Evaporation can be esimated but is difficult, discharge can be considered known: \n",
    "\n",
    "$\\frac{\\Delta S}{\\Delta t} = P - Q - ET - GW$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cc6abb-15cb-4123-a954-10425a0e658f",
   "metadata": {},
   "source": [
    "# first focus on grace data\n",
    "Downloaded data can be loaded in, note using stokes coefficients till 60th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "524c4e05-4769-4d23-959e-70b54bc8b0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "grace_files = glob.glob('Data\\\\Grace\\*.gfc')\n",
    "love_numbers_kl = np.loadtxt('Data\\\\loadLoveNumbers_60.txt')[:,1]\n",
    "l = 60\n",
    "m = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cb9398-3049-40d4-9e4a-59bedd4ec7f5",
   "metadata": {},
   "source": [
    "Stokes coeffcients order 1& 2 need to be handeled differently\n",
    "\n",
    "order 1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81738dc2-fe97-4479-b75b-d186634a7015",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_stoke_coeff1 = f'Data\\\\Grace\\\\degree1_stokes_coeff.txt'\n",
    "df_stokes_1 = pd.read_csv(fname_stoke_coeff1,skiprows=116,delimiter=\" \", \n",
    "        names=['GRCOF2',\"_1\",\"_2\",\"_3\",\"l\",\"_4\",\"_5\",\"m\", \"Clm\",\"Slm\",\"sd_Clm\",\"sd_Slm\",\"begin_date\",\"end_date\"])\n",
    "### drop unwanted\n",
    "df_stokes_1.drop(columns=[\"_1\",\"_2\",\"_3\",\"_4\",\"_5\",\"GRCOF2\",\"sd_Clm\",\"sd_Slm\"],inplace=True)\n",
    "### Reformat dates\n",
    "df_stokes_1[\"end_date\"] = df_stokes_1.apply(lambda x: pd.Timestamp(f'{str(x.end_date)[0:4]}-{str(x.end_date)[4:6]}-{str(x.end_date)[6:8]}'),axis=1)\n",
    "### dirty fix to make more fit\n",
    "df_stokes_1[\"begin_date\"] = df_stokes_1.apply(lambda x: pd.Timestamp(f'{str(x.begin_date)[0:4]}-{str(x.begin_date)[4:6]}-01'),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "536518bb-2a6f-48e8-9c5c-31cae3d9bf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_0 = df_stokes_1[df_stokes_1['m'] == 0].set_index('begin_date')\n",
    "df_1_1 = df_stokes_1[df_stokes_1['m'] == 1].set_index('begin_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5621d2d9-215d-414e-9d4f-8b5468655edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# easiest way to fix the indexing in matchting the correct indexes\n",
    "df_index_replace = df_1_1.index.to_numpy().copy()\n",
    "for i, index in enumerate(df_1_1.index):\n",
    "    if i == 0:\n",
    "        pass\n",
    "    if (df_1_1.index[i-1] - index) == pd.Timedelta(0):\n",
    "        replace = pd.Timestamp(f'{index.year}-{index.month+1}-{index.day}')\n",
    "        df_index_replace[i] = replace\n",
    "df_1_1.index = df_index_replace\n",
    "df_1_0.index = df_index_replace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e8c994-e032-4cd4-a919-7d25e4e577df",
   "metadata": {},
   "source": [
    "order 2,0 & 3,0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c240b506-2395-42e7-8614-0b00667c96da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MJD_to_ts(mjd):\n",
    "    ## thanks to https://stackoverflow.com/questions/72597699/how-do-i-convert-mjdjulian-date-to-utc-using-astropy\n",
    "    # Start with some time in modified julian date (MJD)\n",
    "    # Convert to Julian Date\n",
    "    mjd = float(mjd)\n",
    "    jd = mjd + 2400000.5\n",
    "    # Convert to astropy Time object\n",
    "    t = astropy.time.Time(jd, format='jd')\n",
    "    # Convert to datetime\n",
    "    str = t.to_datetime()\n",
    "    return str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be465350-bf18-4e21-ba84-93137812180f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_stoke_coeff2 = f'Data\\\\Grace\\\\degree2_stokes_coeff.txt'\n",
    "col_names = ['MJD begin',\"Year fraction begin\",\"C20\",\"C20 - C20_mean (1.0E-10)\",\"sig_C20 (1.0E-10)\", \n",
    "             \"C30\",\"C30 - C30_mean (1.0E-10)\",\"sig_C30 (1.0E-10)\",'MJD end',\"Year fraction end\"]\n",
    "df_stokes_2_3 = pd.read_csv(fname_stoke_coeff2,skiprows=37,delimiter=\"\\s+\",names=col_names)\n",
    "# fix date format\n",
    "df_stokes_2_3[\"begin_date\"] = df_stokes_2_3.apply(lambda x: MJD_to_ts(x['MJD begin']), axis=1)\n",
    "df_stokes_2_3[\"end_date\"] = df_stokes_2_3.apply(lambda x: MJD_to_ts(x['MJD begin']), axis=1)\n",
    "df_stokes_2_3 = df_stokes_2_3[[\"begin_date\",\"C20\",\"C30\",\"end_date\"]].set_index(\"begin_date\")\n",
    "\n",
    "# allign indexes and replace like in C_1_1..\n",
    "df_stokes_2_3 = df_stokes_2_3.iloc[:-2] # remove last two months to make same length\n",
    "df_stokes_2_3.index = df_index_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56242704-6c88-4c9e-81cb-28e44a23ecef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names and dates needed, obtained from file name\n",
    "grace_names = [file[-11:-4] for file in grace_files]\n",
    "times = [pd.Timestamp(grace_names_i) for grace_names_i in grace_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0acbf7c5-7ffd-42b2-b3ae-7edd4f3590e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12,5))\n",
    "# plt.plot(times,marker=\".\",lw=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "857d684a-a022-4bb3-84b3-feb948c75133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-06-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# load in all coefficient\n",
    "C, S = [], []\n",
    "for i, file in enumerate(grace_files):\n",
    "    C_i, S_i, R, GM = readstrokescoefficients(file)\n",
    "    \n",
    "    # replace C_1_0,C_1_1, S_1_1, C_2_0, C_3_0\n",
    "    try:\n",
    "        test  = df_1_0.loc[times[i],\"Clm\"]\n",
    "        new_time = times[i]\n",
    "    except KeyError: # issue with finding correct value, this is easiest fix\n",
    "        new_time = pd.Timestamp(f'{times[i].year}-{times[i].month-1}-{times[i].day}')\n",
    "        print(new_time)\n",
    "    \n",
    "    C_i[1,0]  = df_1_0.loc[new_time,\"Clm\"]\n",
    "    C_i[1,1]  = df_1_1.loc[new_time,\"Clm\"]\n",
    "    S_i[1,1]  = df_1_1.loc[new_time,\"Slm\"]\n",
    "    C_i[2,0]  = df_stokes_2_3.loc[new_time,\"C20\"]\n",
    "    C_30  = df_stokes_2_3.loc[new_time,\"C30\"]\n",
    "    if np.isnan(C_30): \n",
    "        pass\n",
    "    else: \n",
    "        C_i[3,0] = C_30\n",
    "        \n",
    "    C.append(C_i), S.append(S_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8255f383-0dd3-46a8-bf36-e29b39e30414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate means of coefficients\n",
    "C = np.array(C)\n",
    "S = np.array(S)\n",
    "C_mean = C.sum(axis=0)/len(C)\n",
    "S_mean = S.sum(axis=0)/len(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ae106bc-dc6d-4d3d-902d-df818307ef2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove mean coefficients\n",
    "dc1_store = []\n",
    "ds1_store = []\n",
    "for i, c in enumerate(C):\n",
    "    dc1 = C[i] - C_mean\n",
    "    ds1 = S[i] - S_mean\n",
    "    dc1_store.append(dc1)\n",
    "    ds1_store.append(ds1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6772a4f4-eb40-442e-accd-f0dd1bc91b06",
   "metadata": {},
   "source": [
    "using \n",
    "\\begin{equation}\n",
    "\\delta h_w \\left(\\theta,\\lambda\\right) = \\sum_{l,m=0}^{\\infty}=\\bar{C_{lm}^{\\delta h_w}} \\bar{Y}_{lm}\\left(\\theta,\\lambda\\right)\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60b8f7cf-74ae-46bb-93b8-4b8cad9a0ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rho_av__rho_w = 5.5 # aprox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a594984b-e0b7-4088-9bda-50da04317cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dh_c1_store = []\n",
    "dh_s1_store = []\n",
    "\n",
    "for k, dc1 in enumerate(dc1_store):\n",
    "    C_dhw_i_1 = np.zeros((l+1, l+1))\n",
    "    S_dhw_i_1 = np.zeros((l+1, l+1))\n",
    "    for i in range(l+1):\n",
    "        for j in range(i+1):\n",
    "            multiplication_factor = (R  * (2 * i + 1) * rho_av__rho_w) / ( 3 * (1 + love_numbers_kl[i]))\n",
    "            C_dhw_i_1[i, j] = (dc1_store[k][i, j] * multiplication_factor)\n",
    "            S_dhw_i_1[i, j] = (ds1_store[k][i, j] * multiplication_factor)\n",
    "    dh_c1_store.append(C_dhw_i_1)\n",
    "    dh_s1_store.append(S_dhw_i_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312fc3c3-0e6c-41df-aa05-aab739d6a590",
   "metadata": {},
   "source": [
    "Create array of lat lon for the area: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c418938-806b-4219-adbe-b16c2e7ec9ec",
   "metadata": {},
   "source": [
    "![Figures\\Lena_Basin_map.png](Figures\\Lena_Basin_map.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b938f906-5f05-43d5-a8f3-430e35cbc6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_lambda= [ 90.  91.  92.  93.  94.  95.  96.  97.  98.  99. 100. 101. 102. 103.\n",
      " 104. 105. 106. 107. 108. 109. 110. 111. 112. 113. 114. 115. 116. 117.\n",
      " 118. 119. 120. 121. 122. 123. 124. 125. 126. 127. 128. 129. 130. 131.\n",
      " 132. 133. 134. 135. 136. 137. 138. 139. 140. 141. 142. 143. 144. 145.\n",
      " 146. 147. 148. 149.]\n",
      "\n",
      "\n",
      "theta= [40. 39. 38. 37. 36. 35. 34. 33. 32. 31. 30. 29. 28. 27. 26. 25. 24. 23.\n",
      " 22. 21. 20. 19. 18. 17. 16. 15. 14. 13. 12. 11.]\n"
     ]
    }
   ],
   "source": [
    "_lambda = np.pi / 180 * np.arange(270, 330, 1) - np.pi  # 90 - 150 # deg lon\n",
    "theta = np.pi - np.pi / 180 * np.arange(180 - 40, 180 - 10, 1)  # 80 - 50 deg lat\n",
    "\n",
    "print('_lambda=',_lambda/np.pi*180)\n",
    "print('\\n')\n",
    "print('theta=',theta/np.pi*180)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9827f93f-afdb-412e-8bf8-95ef49cf1021",
   "metadata": {},
   "source": [
    "This is how the loop looks, to speed it up it is run in parrallel in `multicore_raw.py`, thus here we only actually load in the netCDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e007eb6-141a-45fb-bd93-2ca5fa36a89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-27 11:46:25.223587\n",
      "z=0\n",
      "i=29 (out of 30)\n",
      "z=1\n",
      "i=29 (out of 30)\n",
      "2023-10-27 11:46:47.528662\n"
     ]
    }
   ],
   "source": [
    "times = [pd.Timestamp(grace_names_i) for grace_names_i in grace_names]\n",
    "run = True\n",
    "debug = True\n",
    "if debug:\n",
    "    n=2                   \n",
    "    loop = dh_c1_store[:n]\n",
    "    times = times[:(n)]      # debugging\n",
    "    if run: fname = \"Data\\\\anomally_waterhead_raw_test.nc\"\n",
    "    else: fname = \"Data\\\\anomally_waterhead_raw_test_multicore.nc\"\n",
    "else:\n",
    "    loop = dh_c1_store\n",
    "    if run: fname = \"Data\\\\anomally_waterhead_raw.nc\" \n",
    "    else: fname = \"Data\\\\anomally_waterhead_raw_multicore.nc\"\n",
    "    \n",
    "if run:\n",
    "    # this is a much slower method, faster is the multicore.py which uses multi proecessing \n",
    "    store_ewh_1 = []\n",
    "    print(datetime.datetime.now())\n",
    "    for z, dh_c1 in enumerate(loop): # debugging\n",
    "        print(f'{z=}',end='\\n')\n",
    "        ewh_i_1 = np.zeros((len(theta), len(_lambda)))\n",
    "        ewh_i_2 = np.zeros((len(theta), len(_lambda)))\n",
    "        \n",
    "        for i in range(len(theta)):                              # loop over all thetas\n",
    "            print(f'{i=} (out of {len(theta)})',end='\\r')\n",
    "            P_lm = plm(theta[i], l)                              # all Legendre Functions for one theta\n",
    "            for j in range(len(_lambda)):                        # loop over all lambdas\n",
    "                for k in range(l+1):                             # loop over all degrees\n",
    "                    for t in range(k+1):                         # loop over negative orders\n",
    "                        sin_t_lambda = np.sin(t*_lambda[j])      # negative orders\n",
    "                        cos_t_lambda = np.cos(t*_lambda[j])      # non-negative orders\n",
    "                        # compute here equivalent water heights\n",
    "                        ewh_i_1[i, j] = ewh_i_1[i, j] + (dh_s1_store[z][k, t] * P_lm[k, t] * sin_t_lambda)\n",
    "                        ewh_i_1[i, j] = ewh_i_1[i, j] + (dh_c1_store[z][k, t] * P_lm[k, t] * cos_t_lambda)\n",
    "        print('\\r')\n",
    "        store_ewh_1.append(ewh_i_1)\n",
    "\n",
    "\n",
    "    ds = xr.DataArray(store_ewh_1, dims=(\"time\",\"lat\",\"lon\"),coords={\"lon\":_lambda/np.pi*180,\n",
    "                                                                 \"lat\": 90 - theta/np.pi*180,\n",
    "                                                                 \"time\":times}, name=\"dh(m)\")\n",
    "    \n",
    "    ds.to_netcdf(fname)\n",
    "    print(datetime.datetime.now())\n",
    "else:\n",
    "    ds = xr.open_dataset(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3538c5-dd1f-483c-989e-c3d997d809a0",
   "metadata": {},
   "source": [
    "20 = 5min\n",
    "160 = 50min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baee709-1b12-489d-bbfb-cf39bee9faa6",
   "metadata": {},
   "source": [
    "From here on continue in filterd - could refactor but effort"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
